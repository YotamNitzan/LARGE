{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "latent_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "r-Tdf-fRYMHW",
        "EgEEvb_HlCwb",
        "nQ46S2FYUZpG",
        "m9SIY88YUmnb",
        "ExLfVSGIaIcF",
        "KztcaRPjW22Z",
        "r2pxWMkyfJ5W",
        "4tr7fiby8Z21",
        "4d8ZGgPXYIO3",
        "QK9fpwZwbaAu",
        "mjZJFg58bbK4",
        "IsNsCWi0YkqF",
        "MNzQs-Jlavoq",
        "2_zIkFpcbRde",
        "ElY2Ht8-8iNu"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9JoOBOzUd18"
      },
      "source": [
        "#Latent-Based Regression through GAN Semantics\n",
        "\n",
        "This notebook contains an example playground for the paper: LARGE: Latent-Based Regression through GAN Semantics\n",
        "\n",
        "We currently support both sorting and regression models. Expand the relevant section for more info.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42iAOHfUb2q",
        "cellView": "form"
      },
      "source": [
        "#@title Setup required packages, models. This may take a few minutes.\n",
        "\n",
        "import os\n",
        "\n",
        "data_dir        = os.path.join(\"/content\", \"latent_regression\")\n",
        "celeba_dir      = os.path.join(data_dir, \"celeba_images\")\n",
        "latents_dir     = os.path.join(data_dir, \"latents\")\n",
        "cat_dir         = os.path.join(data_dir, \"afhq_cat_images\")\n",
        "cat_latents_dir = os.path.join(data_dir, \"cat_latents\")\n",
        "\n",
        "# install requirements\n",
        "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html \n",
        "\n",
        "!git clone https://github.com/omertov/encoder4editing.git\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "!pip install ftfy regex tqdm \n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "!git clone -b pre_release_fixes https://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@github.com/YotamNitzan/LARGE.git\n",
        "\n",
        "!gdown --id 1WqhoitsRZ2nfUslIMf5Rg_WWOp2LW2Hu\n",
        "\n",
        "!unzip -q latent_regression.zip\n",
        "!unzip -q latent_regression/celeba_images.zip -d $celeba_dir\n",
        "!unzip -q latent_regression/latents.zip -d $latents_dir\n",
        "!unzip -q latent_regression/afhq_cat_images -d $cat_dir\n",
        "!unzip -q latent_regression/afhq_cat_latents -d $cat_latents_dir\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8dh9ilpHo3s",
        "cellView": "form"
      },
      "source": [
        "#@title General imports and variables\n",
        "from PIL import Image\n",
        "\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "set_feature_directions = {\"celebA\": \"precomputed_feature_dirs.npy\",\n",
        "                          \"afhq_cat\": \"precomputed_cat_feature_dirs.npy\"}\n",
        "\n",
        "set_num_layers = {\"celebA\": 18,\n",
        "                  \"afhq_cat\": 16}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Tdf-fRYMHW"
      },
      "source": [
        "# Sorting\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfBwTUAAGVNt"
      },
      "source": [
        "Sorting in broken into two parts:\n",
        "\n",
        "\n",
        "1.   Choose a semantic direction. Currently supported options are:\n",
        "    *   Choose a direction from the paper\n",
        "    *   Create your own using a StyleCLIP-based approach\n",
        "    *   Upload a boundary file\n",
        "\n",
        "2.   Run the sorting script."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgEEvb_HlCwb"
      },
      "source": [
        "## 1) Choose a latent direction:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ46S2FYUZpG"
      },
      "source": [
        "### Precomputed paper directions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_o7IUGMSTRu",
        "cellView": "form"
      },
      "source": [
        "#@title Precomputed boundaries:\n",
        "\n",
        "boundary = 'celebA_hair_color' #@param['celebA_hair_length', 'celebA_hair_color', 'celebA_makeup', 'celebA_expression', 'cat_age', 'cat_yaw', 'cat_pitch']\n",
        "\n",
        "dataset = boundary.split(\"_\")[0]\n",
        "if dataset == \"cat\":\n",
        "    dataset = \"afhq_cat\"\n",
        "\n",
        "clip_boundary_dir = os.path.join(data_dir, \"clip_boundaries\")\n",
        "sefa_boundary_dir = os.path.join(data_dir, \"sefa_boundaries\")\n",
        "\n",
        "boundary_path = {\"celebA_hair_length\": os.path.join(clip_boundary_dir, \"short_to_long_S_10\"),\n",
        "                 \"celebA_hair_color\":  os.path.join(clip_boundary_dir, \"black_to_blonde_S\"),\n",
        "                 \"celebA_makeup\":      os.path.join(clip_boundary_dir, \"makeup_S\"),\n",
        "                 \"celebA_expression\":  os.path.join(clip_boundary_dir, \"sad_to_smile_S\"),\n",
        "                 \"cat_age\":            os.path.join(sefa_boundary_dir, \"cat_age\"),\n",
        "                 \"cat_yaw\":            os.path.join(sefa_boundary_dir, \"cat_yaw\"),\n",
        "                 \"cat_pitch\":          os.path.join(sefa_boundary_dir, \"cat_pitch\")}[boundary]\n",
        "\n",
        "boundary_type = {\"celebA\": \"s\", \"afhq_cat\": \"w\"}[dataset]\n",
        "latent_src = \"e4e\"\n",
        "latent_type = {\"celebA\": \"s_latents\", \"afhq_cat\": \"latents\"}[dataset]\n",
        "boundary_ext = {\"celebA\": \".pickle\", \"afhq_cat\": \".npy\"}[dataset]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9SIY88YUmnb"
      },
      "source": [
        "### Create your own direction with StyleCLIP:\n",
        "\n",
        "*   Enter source and target texts which describe the sorting direction.\n",
        "*   Choose a cutoff value for considered directions. A higher value gives a more disentangled direction, at the risk of removing too much relevant information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo8XRRpCSZBt",
        "cellView": "form",
        "outputId": "cf5f5867-b364-46a8-9377-3a74174f1d16"
      },
      "source": [
        "#@title Generate a StyleCLIP boundary:\n",
        "\n",
        "dataset = 'afhq_cat' #@param['celebA', 'afhq_cat']\n",
        "\n",
        "source_text = \"white fur\" #@param {type: \"string\"}\n",
        "target_text = \"black fur\" #@param {type: \"string\"}\n",
        "\n",
        "cutoff_percentile = 80 #@param {type: \"integer\"}\n",
        "\n",
        "clip_boundary_dir = os.path.join(data_dir, \"clip_boundaries\")\n",
        "\n",
        "feature_dir_file = set_feature_directions[dataset]\n",
        "\n",
        "num_layers = set_num_layers[dataset]\n",
        "\n",
        "feature_dirs_file = os.path.join(clip_boundary_dir, feature_dir_file)\n",
        "\n",
        "boundary = \"live_editing\"\n",
        "\n",
        "!python LARGE/get_clip_boundary.py --source_text \"$source_text\" --target_text \"$target_text\" --name $boundary --precomputed_dirs $feature_dirs_file --out_dir $clip_boundary_dir --cutoff_percentile $cutoff_percentile --model_layers $num_layers\n",
        "\n",
        "boundary_path = os.path.join(clip_boundary_dir, boundary)\n",
        "\n",
        "print(f\"Done! The generated boundary can be found in: {boundary_path}\")\n",
        "\n",
        "boundary_type = \"w+\"\n",
        "latent_src = \"psp\"\n",
        "latent_type = \"latents\"\n",
        "boundary_ext = \".npy\""
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"LARGE/get_clip_boundary.py\", line 11, in <module>\n",
            "    import matplotlib.pyplot as plt\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 2282, in <module>\n",
            "    switch_backend(rcParams[\"backend\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\", line 221, in switch_backend\n",
            "    backend_mod = importlib.import_module(backend_name)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/__init__.py\", line 2, in <module>\n",
            "    from .connect import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/ipykernel/connect.py\", line 18, in <module>\n",
            "    import jupyter_client\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jupyter_client/__init__.py\", line 4, in <module>\n",
            "    from .connect import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/jupyter_client/connect.py\", line 24, in <module>\n",
            "    import zmq\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/__init__.py\", line 106, in <module>\n",
            "    from zmq import sugar\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/sugar/__init__.py\", line 7, in <module>\n",
            "    from zmq.sugar import constants, context, frame, poll, socket, tracker, version\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/zmq/sugar/constants.py\", line 8, in <module>\n",
            "    from zmq.utils.constant_names import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
            "KeyboardInterrupt\n",
            "Done! The generated boundary can be found in: /content/latent_regression/clip_boundaries/live_editing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExLfVSGIaIcF"
      },
      "source": [
        "### Upload your own boundary:\n",
        "\n",
        "The boundary should describe a direction in one of the latent spaces of StyleGAN2. We currently support boundaries in a numpy (npy) and pickle format, and in either W, W+ or S space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIU_nJKaaHsw",
        "cellView": "form"
      },
      "source": [
        "dataset = 'afhq_cat' #@param['celebA', 'afhq_cat']\n",
        "\n",
        "boundary_path = \"\" #@param {type: \"string\"}\n",
        "\n",
        "boundary_type = \"w\" #@param['w', 'w+', 's']\n",
        "latent_src  = \"psp\" if boundary_type == \"w+\" else \"e4e\"\n",
        "latent_type = \"latents\"\n",
        "boundary_ext = boundary_path.split(\".\")[-1]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KztcaRPjW22Z"
      },
      "source": [
        "## 2) Sort!\n",
        "\n",
        "Pick a seed and run the block in order to sort a random subset of CelebA according to your chosen semantic boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "_e56nDdPZCwL",
        "cellView": "form",
        "outputId": "93ef8ad2-ce9d-4a9d-c170-d56c34de6b8e"
      },
      "source": [
        "#@title Sort with the chosen boundary:\n",
        "\n",
        "seed = 2 #@param {type: \"integer\"}\n",
        "\n",
        "latents = cat_latents_dir if dataset == \"afhq_cat\" else os.path.join(latents_dir, latent_src, dataset, latent_type)\n",
        "images  = cat_dir if dataset == \"afhq_cat\" else celeba_dir\n",
        "\n",
        "output_dir = os.path.join(\"sorted\", boundary)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "num_layers = set_num_layers[dataset]\n",
        "\n",
        "to_wp_flag = \"--boundary_to_wp\" if boundary_type == 'w' else ''\n",
        "!python LARGE/sort.py --latent_path $latents --image_path $images --boundary_path $boundary_path --num_samples 10 --out_dir $output_dir --seed $seed --num_for_plot 10 --boundary_file_ext $boundary_ext $to_wp_flag --model_layers $num_layers\n",
        "Image.open(os.path.join(output_dir, \"all_sorted.png\"))\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: sort.py [-h] [--latent_path LATENT_PATH] [--image_path IMAGE_PATH]\n",
            "               --boundary_path BOUNDARY_PATH\n",
            "               [--negative_boundary_path NEGATIVE_BOUNDARY_PATH]\n",
            "               [--latent_file_ext {.pickle,.pkl,.npy}]\n",
            "               [--boundary_file_ext {.pickle,.pkl,.npy}]\n",
            "               [--num_samples NUM_SAMPLES] [--num_for_plot NUM_FOR_PLOT]\n",
            "               --out_dir OUT_DIR [--layer_weights_path LAYER_WEIGHTS_PATH]\n",
            "               [--boundary_to_wp] [--seed SEED] [--model_layers MODEL_LAYERS]\n",
            "sort.py: error: argument --boundary_path: expected one argument\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-35aa8f34df94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mto_wp_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"--boundary_to_wp\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mboundary_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python LARGE/sort.py --latent_path $latents --image_path $images --boundary_path $boundary_path --num_samples 10 --out_dir $output_dir --seed $seed --num_for_plot 10 --boundary_file_ext $boundary_ext $to_wp_flag --model_layers $num_layers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all_sorted.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2843\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2844\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sorted/live_editing/all_sorted.png'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2pxWMkyfJ5W"
      },
      "source": [
        "# Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtWmN7YXFBru"
      },
      "source": [
        "For regression - first train a model using labeled CelebA data, then run inference on images from the CelebA test split or an uploaded image of your own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tr7fiby8Z21"
      },
      "source": [
        "### Train a regression model\n",
        "\n",
        "We currently support pose and age models, using the labels from the paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtm9EJXYfjmp",
        "cellView": "form"
      },
      "source": [
        "model_type = 'pose' #@param['pose', 'age']\n",
        "\n",
        "latents_path = os.path.join(latents_dir, \"e4e\", \"celebA\", \"latents\")\n",
        "\n",
        "annotation_file = {\"pose\": \"celeba_yaw.csv\", \"age\": \"celeba_age2.csv\"}[model_type]\n",
        "boundary_path = {\"pose\": \"pose_boundary\", \"age\": \"clip_boundaries/old_to_young_WP\"}[model_type]\n",
        "attribute = {\"pose\": \"yaw\", \"age\": \"Young\"}[model_type]\n",
        "\n",
        "w_boundary_flags = f\"--boundary_to_wp --layer_weights_path {data_dir}/{boundary_path}/layer_weights.npy\" if model_type == \"pose\" else \"\"\n",
        "distance_type_flags = \"\" if model_type == \"pose\" else \"--distance_type euclidean\"\n",
        "\n",
        "!python LARGE/train_regression_model.py --data-path $latents_path --annotations-file $data_dir/labels/$annotation_file --boundary_path $data_dir/$boundary_path $w_boundary_flags --attribute $attribute --output-dir $data_dir/regression_models/ --regularization L1 L2 --feature_sample_ratio 0.95 $distance_type_flags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d8ZGgPXYIO3"
      },
      "source": [
        "### Invert an image into the latent space of the GAN, or choose a pre-inverted image:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK9fpwZwbaAu"
      },
      "source": [
        "#### Choose a pre-inverted image (skip if you want to invert your own)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGwFcLyMbeRx",
        "cellView": "form"
      },
      "source": [
        "image_files = os.listdir(celeba_dir)\n",
        "image_paths = [os.path.join(celeba_dir, file_name) for file_name in image_files]\n",
        "\n",
        "latent_type = \"w+\" #@param['w+', 's']\n",
        "latent_dir_name = \"s_latents\" if latent_type == 's' else \"latents\"\n",
        "\n",
        "latents = [os.path.join(latents_dir, \"e4e\", \"celebA\", latent_dir_name, file_name.split(\".\")[0] + \".pickle\") for file_name in image_files]\n",
        "\n",
        "image_id = 94 #@param {type:\"slider\", min:1, max:4500, step:1}\n",
        "\n",
        "chosen_latent = latents[image_id - 1]\n",
        "chosen_img = image_paths[image_id - 1]\n",
        "\n",
        "img = Image.open(chosen_img)\n",
        "img.resize((256, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjZJFg58bbK4"
      },
      "source": [
        "#### Invert an image (skip if you picked a pre-inverted image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsNsCWi0YkqF"
      },
      "source": [
        "##### Setup e4e for inversion. Skip if you want to use a pre-inverted image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9E8IwEAmYGxm",
        "cellView": "form"
      },
      "source": [
        "#@title e4e setup\n",
        "e4e_dir = os.path.join(\"/content\", \"encoder4editing\")\n",
        "sys.path.append(e4e_dir)\n",
        "\n",
        "from models.psp import pSp\n",
        "from gdown import download as drive_download\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from argparse import Namespace\n",
        "\n",
        "e4e_model_path = os.path.join(e4e_dir, \"e4e_ffhq_encode.pt\")\n",
        "if not os.path.isfile(e4e_model_path):\n",
        "    drive_download(\"https://drive.google.com/uc?id=1O8OLrVNOItOJoNGMyQ8G8YRTeTYEfs0P\", e4e_model_path, quiet=False)\n",
        "experiment_type = 'ffhq_encode'\n",
        "\n",
        "# os.chdir('/content/encoder4editing')\n",
        "\n",
        "EXPERIMENT_ARGS = {\n",
        "        \"model_path\": e4e_model_path\n",
        "    }\n",
        "EXPERIMENT_ARGS['transform'] = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "resize_dims = (256, 256)\n",
        "\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "\n",
        "# update the training options\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "e4e_net = pSp(opts)\n",
        "e4e_net.eval()\n",
        "e4e_net.cuda()\n",
        "print('Model successfully loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNzQs-Jlavoq"
      },
      "source": [
        "##### Choose and align an image for inversion. Skip if you want to use a pre-inverted image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw6BWJzzax8N",
        "cellView": "form"
      },
      "source": [
        "#@title Align image\n",
        "image_path = \"latent_regression/sample_image/sample_steph.jpg\" #@param {type: \"string\"}\n",
        "original_image = Image.open(image_path)\n",
        "original_image = original_image.convert(\"RGB\")\n",
        "if experiment_type == \"ffhq_encode\" and 'shape_predictor_68_face_landmarks.dat' not in os.listdir():\n",
        "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
        "    !bzip2 -dk shape_predictor_68_face_landmarks.dat.bz2\n",
        "\n",
        "def run_alignment(image_path):\n",
        "  import dlib\n",
        "  from utils.alignment import align_face\n",
        "  predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
        "  aligned_image = align_face(filepath=image_path, predictor=predictor) \n",
        "  print(\"Aligned image has shape: {}\".format(aligned_image.size))\n",
        "  return aligned_image \n",
        "\n",
        "if experiment_type == \"ffhq_encode\":\n",
        "  input_image = run_alignment(image_path)\n",
        "else:\n",
        "  input_image = original_image\n",
        "\n",
        "input_image.resize(resize_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_zIkFpcbRde"
      },
      "source": [
        "##### Invert the aligned image. Skip if you want to use a pre-inverted image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMAfmxGrbD7V",
        "cellView": "form"
      },
      "source": [
        "#@title Invert the image\n",
        "\n",
        "from utils.common import tensor2im\n",
        "\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "def display_alongside_source_image(result_image, source_image):\n",
        "    res = np.concatenate([np.array(source_image.resize(resize_dims)),\n",
        "                          np.array(result_image.resize(resize_dims))], axis=1)\n",
        "    return Image.fromarray(res)\n",
        "\n",
        "def run_on_batch(inputs, net):\n",
        "    images, latents = net(inputs.to(\"cuda\").float(), randomize_noise=False, return_latents=True)\n",
        "    if experiment_type == 'cars_encode':\n",
        "        images = images[:, :, 32:224, :]\n",
        "    return images, latents\n",
        "\n",
        "with torch.no_grad():\n",
        "    images, latents = run_on_batch(transformed_image.unsqueeze(0), e4e_net)\n",
        "    result_image, latent = images[0], latents[0]\n",
        "\n",
        "print(latents.size())\n",
        "\n",
        "chosen_latent = os.path.join(\"/content\", \"latent.npy\")\n",
        "chosen_img = image_path\n",
        "\n",
        "np.save(chosen_latent, latents.detach().cpu().numpy())\n",
        "\n",
        "# Display inversion:\n",
        "display_alongside_source_image(tensor2im(result_image), input_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElY2Ht8-8iNu"
      },
      "source": [
        "### Predict semantic property strength using the trained regression model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge261zPL8uwK",
        "cellView": "form"
      },
      "source": [
        "num_training_images = 1000 #@param[2, 5, 10, 20, 1000]\n",
        "\n",
        "model_file = f\"model_{num_training_images}.pickle\"\n",
        "\n",
        "!python LARGE/infer.py --pretrained_model $data_dir/regression_models/$model_file --latent_path \"$chosen_latent\" --boundary_path $data_dir/$boundary_path/boundary.npy $w_boundary_flags $distance_type_flags"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}